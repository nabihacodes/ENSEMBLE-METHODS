# ENSEMBLE-METHODS
This is the glass dataset ,that i have used for applying the ensemble methods of Random Forest ,BAgging and Boosting Methods ,and i have compared the results based on accuracies ,recall score ,F1 and precision score.
</h>Glass Classification Project<h/>
This project focuses on classifying types of glass using machine learning techniques. The dataset used is the Glass Identification dataset, and several ensemble methods were applied and compared, including Random Forest, Bagging, and boosting algorithms such as AdaBoost, XGBoost, and Gradient Boosting.

üìå Project Overview
The goal of this project was to build accurate classification models to predict glass types based on chemical attributes. A significant part of the work involved handling and preparing a messy dataset for modeling.

üîç Data Preprocessing
The dataset posed several challenges:

Class imbalance

Numerous missing values

Presence of outliers

The following preprocessing steps were completed:

Data cleaning and imputation of missing values

Outlier detection and treatment

Feature scaling and transformation

Splitting the data into training and testing sets

Note: While the dataset was imbalanced, no resampling or balancing techniques were applied during modeling.

‚öôÔ∏è Models Used
The following ensemble learning methods were used:

Random Forest Classifier

Bagging Classifier

AdaBoost

XGBoost

Gradient Boosting

‚úÖ Model Tuning
For each ensemble method, hyperparameters were carefully tuned and adjusted based on model performance. Techniques like cross-validation and iterative tuning were used to find the best parameter settings for each model.

üìà Results & Evaluation
All ensemble methods showed consistent performance with the following evaluation metrics:

Accuracy (Train/Test): 86.2%

Precision: 84.5%

Recall: 84.5%

F1-Score: 84.5%

These results reflect the models‚Äô overall strong and balanced performance despite the data's challenges.
